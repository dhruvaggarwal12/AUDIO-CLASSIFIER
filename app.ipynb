{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "librosa is used to deal with sound signals\n",
    "all the files are wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting a graph for a audio sound \n",
    "# filename='UrbanSound8K/audio/fold1/7061-6-0-0.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #used to display graph of the audio signals\n",
    "import IPython.display as ipd\n",
    "import resampy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #setting the size of the plot\n",
    "# plt.figure(figsize=(14,5))\n",
    "# #sampling rate of raw data is required\n",
    "# data,sample_rate=librosa.load(filename)\n",
    "# librosa.display.waveshow(data,sr=sample_rate)\n",
    "# ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample rate in audio how many times is a sound sampled per second\n",
    "librosa normalize all the signals in a single samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata=pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "# metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find if balanced or imbalanced\n",
    "# metadata['class'].value_counts #this gives the class value for each audio\n",
    "# metadata['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING\n",
    "EXTRACT IMPORTANT INFO AND THEN MAKE IT IN INDEPENDENT AND DEPENDENT FEATURES CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_file_path='UrbanSound8K/audio/fold1/7061-6-0-0.wav'\n",
    "# librosa_audio_path,librosa_sample_rate=librosa.load(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(librosa_audio_path)\n",
    "# #librosa convert it into one dimensional signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "# plt.plot(librosa_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy give two channels stereochannel output whereas librosa convert it into single channel normalised\n",
    "\n",
    "# from scipy.io import wavfile as wav\n",
    "# wave_sample_rate,wave_audio=wav.read(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "# plt.plot(wave_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT FEATURES\n",
    "MFCC Mel frequency cepstral coefficients give us features based on frequency and time characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for a single file the features extracted in above cells\n",
    "# mfccs=librosa.feature.mfcc(y=librosa_audio_path,sr=librosa_sample_rate,n_mfcc=40)\n",
    "# #n_mfcc is\n",
    "# print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs #it is for single file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset_path='UrbanSound8K/audio/'\n",
    "import pandas as pd\n",
    "metadata=pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file, res_type='scipy') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3550it [01:32, 53.18it/s]c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "8324it [03:03, 61.46it/s]c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n",
      "8732it [03:11, 45.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients will extract info related to frequency and time data\n",
    "extracted_features=[]\n",
    "#iterrows will traverse through every row\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-211.01833, 61.27989, -121.51646, -62.041412,...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-415.5431, 97.27048, -40.93115, 49.012497, 11...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-450.98102, 110.367256, -35.585976, 41.206608...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-405.06644, 89.19861, -23.047182, 40.789795, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-438.15137, 101.76034, -40.557262, 48.588905,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-439.66586, 106.916115, -23.77482, 45.515587,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-468.0223, 107.30417, -16.481619, 44.069992, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-455.72733, 104.52276, -17.119024, 37.868294,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-464.89252, 116.19179, -27.944195, 48.562275,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-187.0852, 100.292435, -0.25078622, -12.96382...</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[-193.94281, 97.157776, -14.413033, 0.7761316,...</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[-198.61028, 77.4126, -11.470458, 15.699568, -...</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[-194.66199, 88.85493, -15.285444, 12.95386, -...</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[-112.73151, 51.53713, -8.396621, 8.081745, 4....</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[-271.75055, 95.621346, -84.97943, -10.4091, 5...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature             class\n",
       "0   [-211.01833, 61.27989, -121.51646, -62.041412,...          dog_bark\n",
       "1   [-415.5431, 97.27048, -40.93115, 49.012497, 11...  children_playing\n",
       "2   [-450.98102, 110.367256, -35.585976, 41.206608...  children_playing\n",
       "3   [-405.06644, 89.19861, -23.047182, 40.789795, ...  children_playing\n",
       "4   [-438.15137, 101.76034, -40.557262, 48.588905,...  children_playing\n",
       "5   [-439.66586, 106.916115, -23.77482, 45.515587,...  children_playing\n",
       "6   [-468.0223, 107.30417, -16.481619, 44.069992, ...  children_playing\n",
       "7   [-455.72733, 104.52276, -17.119024, 37.868294,...  children_playing\n",
       "8   [-464.89252, 116.19179, -27.944195, 48.562275,...  children_playing\n",
       "9   [-187.0852, 100.292435, -0.25078622, -12.96382...          car_horn\n",
       "10  [-193.94281, 97.157776, -14.413033, 0.7761316,...          car_horn\n",
       "11  [-198.61028, 77.4126, -11.470458, 15.699568, -...          car_horn\n",
       "12  [-194.66199, 88.85493, -15.285444, 12.95386, -...          car_horn\n",
       "13  [-112.73151, 51.53713, -8.396621, 8.081745, 4....          car_horn\n",
       "14  [-271.75055, 95.621346, -84.97943, -10.4091, 5...          dog_bark"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting it into a data frame\n",
    "import pandas as pd\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "Y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_one_hot: (8732, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform Y\n",
    "y_labels = labelencoder.fit_transform(Y)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "Y = to_categorical(y_labels)\n",
    "\n",
    "# Check the shape of the one-hot encoded labels\n",
    "print(\"Shape of y_one_hot:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (8732, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Shape of Y:\", np.shape(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6985, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6985, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# model=Sequential()\n",
    "\n",
    "# ##first layer\n",
    "# model.add(Dense(100,input_shape=(40,0)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# ##second layer\n",
    "# model.add(Dense(200))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# ##third layer\n",
    "# model.add(Dense(100))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# ###final layer\n",
    "# model.add(Dense(num_labels))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(100, activation='relu', input_shape=(40,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')  # Change to 10 units for classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m4,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m20,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,410</span> (177.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,410\u001b[0m (177.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,410</span> (177.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,410\u001b[0m (177.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1148 - loss: 23.3683\n",
      "Epoch 1: val_loss improved from inf to 2.29081, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1149 - loss: 22.8819 - val_accuracy: 0.1162 - val_loss: 2.2908\n",
      "Epoch 2/20\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1264 - loss: 2.6659\n",
      "Epoch 2: val_loss improved from 2.29081 to 2.27859, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1263 - loss: 2.6633 - val_accuracy: 0.1128 - val_loss: 2.2786\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1124 - loss: 2.3644\n",
      "Epoch 3: val_loss improved from 2.27859 to 2.26968, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1124 - loss: 2.3643 - val_accuracy: 0.1133 - val_loss: 2.2697\n",
      "Epoch 4/20\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1137 - loss: 2.3124\n",
      "Epoch 4: val_loss improved from 2.26968 to 2.26479, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1137 - loss: 2.3122 - val_accuracy: 0.1133 - val_loss: 2.2648\n",
      "Epoch 5/20\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1113 - loss: 2.2773\n",
      "Epoch 5: val_loss improved from 2.26479 to 2.26163, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1114 - loss: 2.2774 - val_accuracy: 0.1053 - val_loss: 2.2616\n",
      "Epoch 6/20\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1217 - loss: 2.2731\n",
      "Epoch 6: val_loss improved from 2.26163 to 2.25964, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1211 - loss: 2.2729 - val_accuracy: 0.1076 - val_loss: 2.2596\n",
      "Epoch 7/20\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1163 - loss: 2.2676\n",
      "Epoch 7: val_loss improved from 2.25964 to 2.25775, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1163 - loss: 2.2677 - val_accuracy: 0.1093 - val_loss: 2.2577\n",
      "Epoch 8/20\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1158 - loss: 2.2608\n",
      "Epoch 8: val_loss improved from 2.25775 to 2.25022, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1159 - loss: 2.2609 - val_accuracy: 0.1139 - val_loss: 2.2502\n",
      "Epoch 9/20\n",
      "\u001b[1m202/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1221 - loss: 2.2617\n",
      "Epoch 9: val_loss improved from 2.25022 to 2.24063, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1220 - loss: 2.2615 - val_accuracy: 0.1151 - val_loss: 2.2406\n",
      "Epoch 10/20\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1259 - loss: 2.2453\n",
      "Epoch 10: val_loss improved from 2.24063 to 2.23322, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1262 - loss: 2.2455 - val_accuracy: 0.1322 - val_loss: 2.2332\n",
      "Epoch 11/20\n",
      "\u001b[1m212/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1323 - loss: 2.2477\n",
      "Epoch 11: val_loss improved from 2.23322 to 2.22431, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1323 - loss: 2.2476 - val_accuracy: 0.1391 - val_loss: 2.2243\n",
      "Epoch 12/20\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1450 - loss: 2.2266\n",
      "Epoch 12: val_loss improved from 2.22431 to 2.21635, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1449 - loss: 2.2266 - val_accuracy: 0.1420 - val_loss: 2.2163\n",
      "Epoch 13/20\n",
      "\u001b[1m199/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1473 - loss: 2.2130\n",
      "Epoch 13: val_loss did not improve from 2.21635\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1473 - loss: 2.2140 - val_accuracy: 0.1425 - val_loss: 2.2173\n",
      "Epoch 14/20\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1534 - loss: 2.2075\n",
      "Epoch 14: val_loss improved from 2.21635 to 2.18779, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1532 - loss: 2.2071 - val_accuracy: 0.1631 - val_loss: 2.1878\n",
      "Epoch 15/20\n",
      "\u001b[1m197/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1694 - loss: 2.1961\n",
      "Epoch 15: val_loss did not improve from 2.18779\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1684 - loss: 2.1960 - val_accuracy: 0.1614 - val_loss: 2.1915\n",
      "Epoch 16/20\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1661 - loss: 2.1913\n",
      "Epoch 16: val_loss improved from 2.18779 to 2.18366, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1659 - loss: 2.1924 - val_accuracy: 0.1557 - val_loss: 2.1837\n",
      "Epoch 17/20\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1664 - loss: 2.1884\n",
      "Epoch 17: val_loss improved from 2.18366 to 2.16537, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1668 - loss: 2.1879 - val_accuracy: 0.1740 - val_loss: 2.1654\n",
      "Epoch 18/20\n",
      "\u001b[1m202/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1812 - loss: 2.1658\n",
      "Epoch 18: val_loss improved from 2.16537 to 2.12646, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1816 - loss: 2.1656 - val_accuracy: 0.2118 - val_loss: 2.1265\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1965 - loss: 2.1485\n",
      "Epoch 19: val_loss improved from 2.12646 to 2.10687, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1965 - loss: 2.1485 - val_accuracy: 0.2055 - val_loss: 2.1069\n",
      "Epoch 20/20\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1840 - loss: 2.1606\n",
      "Epoch 20: val_loss improved from 2.10687 to 2.04706, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1845 - loss: 2.1595 - val_accuracy: 0.2290 - val_loss: 2.0471\n",
      "Training completed in time:  0:00:18.250464\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "# Define the number of epochs and batch size\n",
    "num_epochs = 20\n",
    "num_batch_size = 32\n",
    "\n",
    "# Set up the ModelCheckpoint callback to save the best model during training\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.keras', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "# Start timing the training process\n",
    "start = datetime.now()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=num_batch_size, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_data=(X_test, Y_test), \n",
    "                    callbacks=[checkpointer], \n",
    "                    verbose=1)\n",
    "\n",
    "# Calculate the duration of the training process\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6985, 40)\n",
      "(6985, 10)\n",
      "(1747, 40)\n",
      "(1747, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy=model.evaluate(X_test,Y_test,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.11545969, 0.05352606, 0.12629654, 0.1266061 , 0.09303439,\n",
       "        0.09823725, 0.04586007, 0.10289791, 0.11954343, 0.11853857]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='UrbanSound8K/audio/fold1/7061-6-0-0.wav'\n",
    "prediction_feature=features_extractor(filename)\n",
    "prediction_feature=prediction_feature.reshape(1,-1)\n",
    "model.predict(prediction_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2289639413356781\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS THE NEW AUDIO DATA\n",
    "PREDICT THE CLASSES\n",
    "INVERSE TRANSFORM YOUR PREDICTED LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-70.26146     46.57773     17.933352    12.837602    -2.4973214\n",
      "  11.500587     5.6505437   10.455527     3.02687      4.900912\n",
      "  -1.0296725    0.22962235   1.4400967    4.0238686   -1.761962\n",
      "  -4.1826906   -4.0614576   -2.490382    -2.7343123   -1.5911713\n",
      "  -0.8485845    0.09690225  -0.8171117    1.6531094    0.48117268\n",
      "   0.6096463   -0.11275851  -0.97154903   0.18284698   0.09701674\n",
      "  -1.1354079   -2.791831    -1.6149555    0.37015003   1.4861481\n",
      "   0.69163895   0.4293873   -1.4722515   -0.45647535  -0.2672316 ]\n",
      "[[-70.26146     46.57773     17.933352    12.837602    -2.4973214\n",
      "   11.500587     5.6505437   10.455527     3.02687      4.900912\n",
      "   -1.0296725    0.22962235   1.4400967    4.0238686   -1.761962\n",
      "   -4.1826906   -4.0614576   -2.490382    -2.7343123   -1.5911713\n",
      "   -0.8485845    0.09690225  -0.8171117    1.6531094    0.48117268\n",
      "    0.6096463   -0.11275851  -0.97154903   0.18284698   0.09701674\n",
      "   -1.1354079   -2.791831    -1.6149555    0.37015003   1.4861481\n",
      "    0.69163895   0.4293873   -1.4722515   -0.45647535  -0.2672316 ]]\n",
      "(1, 40)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted label index: [5]\n",
      "Predicted class: ['engine_idling']\n"
     ]
    }
   ],
   "source": [
    "# filename='UrbanSound8K/audio/fold1/7061-6-0-0.wav'\n",
    "# audio, sample_rate = librosa.load(filename, res_type='scipy') \n",
    "# mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "# mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "# print(mfccs_scaled_features)\n",
    "# mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "# print(mfccs_scaled_features)\n",
    "# print(mfccs_scaled_features.shape)\n",
    "# predicted_label=model.predict(mfccs_scaled_features)\n",
    "# print(predicted_label)\n",
    "# prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "# prediction_class.reshape(1,-1)\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load audio and extract features\n",
    "filename = 'UrbanSound8K/audio/fold4/7064-6-0-0.wav'\n",
    "audio, sample_rate = librosa.load(filename, res_type='scipy') \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "\n",
    "print(mfccs_scaled_features)\n",
    "#1 row with 40 features\n",
    "mfccs_scaled_features = mfccs_scaled_features.reshape(1, -1)\n",
    "print(mfccs_scaled_features)\n",
    "print(mfccs_scaled_features.shape)\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(mfccs_scaled_features)\n",
    "\n",
    "# Get the class index with the highest probability\n",
    "#which class it belongs 0 1 ,2 3 there are 0-9 different classes\n",
    "predicted_label_index = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted label index:\", predicted_label_index)\n",
    "\n",
    "# Convert index to class label\n",
    "# Assuming labelencoder is already fit with your class labels\n",
    "#to the name\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label_index)\n",
    "print(\"Predicted class:\", prediction_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
